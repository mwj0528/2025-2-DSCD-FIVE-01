from dotenv import load_dotenv
import os
from langchain_neo4j import Neo4jGraph
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores.neo4j_vector import Neo4jVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from sentence_transformers import CrossEncoder
import torch

from typing import List, Dict


class GraphRAG:
    """HS Code ì¶”ì²œì„ ìœ„í•œ Graph RAG í´ëž˜ìŠ¤"""
    
    def __init__(self, use_graph_rerank: bool = False, graph_rerank_model: str = None, graph_rerank_top_m: int = 5):
        """GraphRAG ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”"""
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        self.NEO4J_URI = os.getenv("NEO4J_URI")
        self.NEO4J_USER = os.getenv("NEO4J_USER")
        self.NEO4J_PASS = os.getenv("NEO4J_PASS")
        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        self.INDEX_NAME = os.getenv("INDEX_NAME")
        self.DEFAULT_GRAPH_RERANK_MODEL = os.getenv("GRAPH_RERANK_MODEL", "BAAI/bge-reranker-v2-m3")
        self.use_graph_rerank = bool(use_graph_rerank)
        self.graph_rerank_model = graph_rerank_model or self.DEFAULT_GRAPH_RERANK_MODEL
        try:
            self.graph_rerank_top_m = max(1, int(graph_rerank_top_m))
        except Exception:
            self.graph_rerank_top_m = 5
        
        # Neo4j Graph ì—°ê²°
        self.graph = Neo4jGraph(
            url=self.NEO4J_URI, 
            username=self.NEO4J_USER, 
            password=self.NEO4J_PASS
        )
        
        # Vector DB ì„¤ì •
        self.MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        self.embedding_model = SentenceTransformerEmbeddings(model_name=self.MODEL_NAME,
                                encode_kwargs={"normalize_embeddings": True})
        
        # Neo4j Vector DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.neo4j_vector_db = Neo4jVector.from_existing_graph(
            embedding=self.embedding_model,
            url=self.NEO4J_URI,
            username=self.NEO4J_USER,
            password=self.NEO4J_PASS,
            index_name=self.INDEX_NAME,          
            node_label="HSItem",            
            text_node_properties=["description"],
            embedding_node_property="embedding",
        )

        # ì„ íƒì  ReRank ì´ˆê¸°í™”
        self._reranker = None
        if self.use_graph_rerank:
            try:
                device = "cuda" if torch.cuda.is_available() else "cpu"
                self._reranker = CrossEncoder(self.graph_rerank_model, device=device)
            except Exception as e:
                print(f"ê²½ê³ : Graph ReRank ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._reranker = None

    def get_vector_candidates(self, user_query: str, k: int = 5) -> List[str]:
        """ìœ ì‚¬ë„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ 4/6ìžë¦¬ ì½”ë“œë§Œ í•„í„°ë§ í›„ ìµœëŒ€ kê°œ ë°˜í™˜.
        ë¶€ì¡±í•˜ë©´ ì ì¦ì ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ kê°œë¥¼ ìµœëŒ€í•œ ì±„ìš´ë‹¤.
        """
        current_fetch = max(20, k * 5)
        max_fetch = max(100, k * 50)
        last_count = -1

        while True:
            results = self.neo4j_vector_db.similarity_search(user_query, k=current_fetch)

            # ê¸°ë³¸: ìœ ì‚¬ë„ ìˆœì„œ ìœ ì§€í•˜ë©° í•„í„°ë§ + ì¤‘ë³µ ì œê±°
            base_ordered: List[str] = []
            seen: set = set()
            filtered_docs = []  # (code, text)
            for doc in results:
                code = doc.metadata.get("code")
                if not code:
                    continue
                if len(code) not in (4, 6):
                    continue
                text = getattr(doc, "page_content", "")
                if code not in seen:
                    seen.add(code)
                    base_ordered.append(code)
                filtered_docs.append((code, text))

            # ReRank í™œì„±í™” ì‹œ CrossEncoderë¡œ ìž¬ì •ë ¬
            if self.use_graph_rerank and self._reranker is not None and filtered_docs:
                try:
                    # ê°™ì€ ì½”ë“œê°€ ì—¬ëŸ¬ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ë©´ ìµœê³  ì ìˆ˜ ì±„íƒ
                    code_to_best_score = {}
                    pairs = [(user_query, text) for _, text in filtered_docs]
                    scores = self._reranker.predict(pairs)
                    for (code, _), s in zip(filtered_docs, scores):
                        score = float(s) if s is not None else 0.0
                        if code not in code_to_best_score or score > code_to_best_score[code]:
                            code_to_best_score[code] = score
                    reranked = sorted(code_to_best_score.items(), key=lambda x: x[1], reverse=True)
                    # ìƒìœ„ graph_rerank_top_më¥¼ ìš°ì„  ì‚¬ìš©í•˜ë˜ ìµœì¢… ë°˜í™˜ì€ ìµœëŒ€ kê°œê¹Œì§€
                    reranked_codes = [c for c, _ in reranked][:max(self.graph_rerank_top_m, k)]
                    # ë¶€ì¡± ì‹œ ê¸°ë³¸ ìˆœì„œë¡œ ë³´ì¶©
                    for c in base_ordered:
                        if len(reranked_codes) >= k:
                            break
                        if c not in reranked_codes:
                            reranked_codes.append(c)
                    if len(reranked_codes) >= k:
                        return reranked_codes[:k]
                    # ê·¸ëž˜ë„ ë¶€ì¡±í•˜ë©´ ê³„ì† fetch í™•ëŒ€
                except Exception as e:
                    print(f"ê²½ê³ : Graph ReRank ì¤‘ ì˜¤ë¥˜: {e}")

            # ReRank ë¯¸ì‚¬ìš© ë˜ëŠ” ë¶ˆì¶©ë¶„ ì‹œ ê¸°ë³¸ ìˆœì„œ ë°˜í™˜ ì‹œë„
            if len(base_ordered) >= k:
                return base_ordered[:k]

            # ë” ê°€ì ¸ì™€ë„ ì¦ê°€ê°€ ì—†ê±°ë‚˜ ìƒí•œ ë„ë‹¬ ì‹œ ì¢…ë£Œ
            if len(results) == last_count or current_fetch >= max_fetch:
                # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°€ëŠ¥í•œ ë§Œí¼ ë°˜í™˜
                if self.use_graph_rerank and self._reranker is not None:
                    # ìœ„ì—ì„œ ì´ë¯¸ ì‹œë„í–ˆìœ¼ë¯€ë¡œ base_ordered ë°˜í™˜
                    return base_ordered
                return base_ordered

            last_count = len(results)
            current_fetch = min(current_fetch * 2, max_fetch)

    def get_graph_context(self, candidate_codes: List[str]) -> str:
        """í›„ë³´ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µ ê²½ë¡œë¥¼ íƒìƒ‰í•˜ê³  LLM Contextë¥¼ ìƒì„±"""
        
        # ðŸš¨ ë™ì  Cypher ì¿¼ë¦¬ ìƒì„±
        # candidates_str = "['8541', '9405']" í˜•íƒœì˜ Cypher ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        candidates_str = str(candidate_codes).replace("'", '"')

        # LLMì´ ì§ì ‘ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , ì½”ë“œë¥¼ ì‚½ìž…í•˜ì—¬ ì‹¤í–‰
        cypher_query = f"""
        UNWIND {candidates_str} AS root_code_str
        MATCH p = (root:HSItem {{code: root_code_str}})-[:HAS_CHILD*1..]->(n)
        WHERE NOT (n)-[:HAS_CHILD]->()
        RETURN nodes(p) AS Path_Nodes, relationships(p) AS Path_Relationships
        """
        
        results = self.graph.query(cypher_query)
        
        final_context = "# [ê²€ìƒ‰ëœ HS Code ê³„ì¸µ êµ¬ì¡° ë°ì´í„°]\n\n"
        
        # --- LLM Context ë¬¸ìžì—´ ë³€í™˜ ë¡œì§ ---
        for result in results:
            nodes = result['Path_Nodes']
            
            # 1. ì‹œê°ì  ê³„ì¸µ ê²½ë¡œ êµ¬ì„± (ê³ ê°ë‹˜ì˜ ì˜ˆì‹œ í˜•íƒœ)
            if not nodes: continue

            path_text = ""
            table_rows = []
            
            for i, node in enumerate(nodes):
                code = node['code']
                desc = node['description']
                
                # ê²½ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
                if i == 0:
                    path_text += f"[ì‹œìž‘ ë…¸ë“œ: {code} ({desc})]\n"
                    level_desc = "ìƒìœ„ ë ˆë²¨"
                elif i == len(nodes) - 1:
                    path_text += f"    |--[:HAS_CHILD]-> [ìµœì¢… ë…¸ë“œ: {code} ({desc})]\n"
                    level_desc = "ìµœì¢… ë ˆë²¨"
                else:
                    path_text += f"    |--[:HAS_CHILD]-> [ì¤‘ê°„ ë…¸ë“œ: {code} ({desc})]\n"
                    level_desc = "ì¤‘ê°„ ë ˆë²¨"
                
                # í…Œì´ë¸” í–‰ ë°ì´í„° ìˆ˜ì§‘
                table_rows.append(f"| {code} | {desc} | {level_desc} |")
            
            # Contextì— ê²½ë¡œ ì¶”ê°€
            final_context += path_text + "\n"
            
            # 2. ì¶”ë¡  ìš”ì•½ í…Œì´ë¸” êµ¬ì„±
            final_context += "---"
            final_context += "\n[ì¶”ë¡  ìš”ì•½ í…Œì´ë¸”]\n"
            final_context += "| ì½”ë“œ | ì˜ë¬¸ í’ˆëª©ëª… | ê³„ì¸µ |\n"
            final_context += "|:---|:---|:---|\n"
            final_context += "\n".join(table_rows) + "\n\n"
        
        return final_context

    def generate_recommendation(self, user_input: str):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ HS Code ì¶”ì²œ ìƒì„±"""
        # 1. Context ê²€ìƒ‰
        context = self.get_graph_context(self.get_vector_candidates(user_input))
        # 2. LLM Prompt êµ¬ì„±
        template = """
        ë‹¹ì‹ ì€ HS Code ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
        ì œê³µëœ [ê²€ìƒ‰ëœ HS Code ê³„ì¸µ êµ¬ì¡° ë°ì´í„°] ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìžì˜ ìƒí’ˆì— ê°€ìž¥ ì í•©í•œ 10ìžë¦¬ HS Codeë¥¼ ì¶”ì²œí•˜ê³ , 
        ì™œ ê·¸ ì½”ë“œë¥¼ ì„ íƒí–ˆëŠ”ì§€ ê³„ì¸µ ê²½ë¡œë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤. 
        ë§Œì•½ ì—¬ëŸ¬ ê²½ë¡œê°€ ê²€ìƒ‰ë˜ì—ˆë‹¤ë©´, ëª¨ë“  ê²½ë¡œë¥¼ ì œì‹œí•˜ê³  ìµœì¢… ì„ íƒì„ ì‚¬ìš©ìžì—ê²Œ ë§¡ê¸°ì‹­ì‹œì˜¤.

        ì‚¬ìš©ìž ìƒí’ˆ: {user_input}

        [ê²€ìƒ‰ëœ HS Code ê³„ì¸µ êµ¬ì¡° ë°ì´í„°]:
        {context}
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # 3. LLM Chain ì‹¤í–‰
        chain = prompt | ChatOpenAI(model="gpt-4-turbo") 
        
        response = chain.invoke({"user_input": user_input, "context": context})
        
        return response.content

    def get_final_context(self, user_input: str, k: int = 5) -> str:
        """
        ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë©”ì¸ ë©”ì„œë“œ
        Inputê³¼ kë¥¼ ë°›ì•„ì„œ ê°€ìž¥ ê°€ê¹Œìš´ Top-k í›„ë³´ì˜ final_contextë¥¼ ë°˜í™˜
        
        Args:
            user_input (str): ì‚¬ìš©ìž ìž…ë ¥ (ìƒí’ˆëª… ë“±)
            k (int): ê²€ìƒ‰í•  í›„ë³´ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            
        Returns:
            str: ê²€ìƒ‰ëœ HS Code ê³„ì¸µ êµ¬ì¡° ë°ì´í„°ì˜ final_context
        """
        # 1. Vector Searchë¡œ í›„ë³´ ì½”ë“œ ê²€ìƒ‰
        candidate_codes = self.get_vector_candidates(user_input, k)
        
        # 2. í›„ë³´ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µ êµ¬ì¡° Context ìƒì„±
        final_context = self.get_graph_context(candidate_codes)
        
        return final_context


# ì‚¬ìš© ì˜ˆì‹œ (ë‹¤ë¥¸ íŒŒì¼ì—ì„œ importí•  ë•ŒëŠ” ì´ ë¶€ë¶„ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
if __name__ == "__main__":
    # GraphRAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    graph_rag = GraphRAG()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("=== GraphRAG í…ŒìŠ¤íŠ¸ ===")
    result = graph_rag.get_final_context("ë°©ë¶€ì²˜ë¦¬í•œ ì ì†¡ ë‚˜ë¬´", k=5)
    print(result)


# final_context = graph_rag.get_final_context(user_input, k) -> context ë°˜í™˜
